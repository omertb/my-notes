source: https://www.katacoda.com/courses/kubernetes

### 1 - Initialise Master
Install kubeadm.

* The command below will initialise the cluster:
```Shell
master $ kubeadm init --token=102952.1a7dd4cc8d1f4cc5 --kubernetes-version $(kubeadm version -o short)
```

> In production, it's recommend to exclude the token causing kubeadm to generate one on your behalf.

* To manage the Kubernetes cluster, the client configuration and certificates are required. This configuration is created when kubeadm initialises the cluster. The command copies the configuration to the users home directory and sets the environment variable for use with the CLI.
```Shell
master $ sudo cp /etc/kubernetes/admin.conf $HOME/
master $ sudo chown $(id -u):$(id -g) $HOME/admin.conf
master $ export KUBECONFIG=$HOME/admin.conf
```

### 2 - Deploy Container Networking Interface (CNI)
The Container Network Interface (CNI) defines how the different nodes and their workloads should
communicate. There are multiple network providers available, some are listed [here](https://kubernetes.io/docs/admin/addons/).

We'll use WeaveWorks:
```Shell
master $ wget "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')" -O weave-kube
master $ kubectl apply -f weave-kube

```
> Weave will now deploy as a series of Pods on the cluster. The status of this can be viewed using the command:
```Shell
master $ kubectl get pod -n kube-system
```

### 3 - Join Cluster
> _Once the Master and CNI has initialised, additional nodes can join the cluster as long as they have the correct token.
 The tokens can be managed via kubeadm token, for example:_ `kubecadm token list`

```Shell
master $ kubeadm token list
TOKEN                     TTL       EXPIRES                USAGES                   DESCRIPTION                       EXTRA GROUPS
102952.1a7dd4cc8d1f4cc5   23h       2019-12-06T16:42:19Z   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token
```
* On the second node, run the command to join the cluster providing the IP address of the Master node.
```Shell
node01 $ kubeadm join --discovery-token-unsafe-skip-ca-verification --token=102952.1a7dd4cc8d1f4cc5 172.17.0.69:6443
```

This is the same command provided after the Master has been initialised.

> The --discovery-token-unsafe-skip-ca-verification` tag is used to bypass the Discovery Token verification. 
As this token is generated dynamically, we couldn't include it within the steps. When in production, use the
token provided by kubeadm init`.`

### 4 - View Nodes

The cluster has now been initialised. The Master node will manage the cluster, while our one worker node will run our
container workloads.

The Kubernetes CLI, known as kubectl, can now use the configuration to access the cluster.
For example, the command below will return the two nodes in our cluster.
```Shell
# kubectl get nodes
```

### 5 - Deploy Pod
The state of the two nodes in the cluster should now be Ready. This means that our deployments can be scheduled and launched.

Using Kubectl, it's possible to deploy pods. Commands are always issued for the Master with each node only responsible for executing the workloads.

* The command below create a Pod based on the Docker Image katacoda/docker-http-server:
```Shell
master $ kubectl create deployment http --image=katacoda/docker-http-server:latest
master $ kubectl get pods
NAME                    READY   STATUS              RESTARTS   AGE
http-7f8cbdf584-ccrtt   0/1     ContainerCreating   0          7s
master $ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
http-7f8cbdf584-ccrtt   1/1     Running   0          10s
```
> Once running, you can see the Docker Container running on the node.
```Shell
node01 $ docker ps | grep docker-http-server
```

### 5 - Deploy Dashboard

* Deploy the dashboard yaml with the command `kubectl apply -f dashboard.yaml`

* The dashboard is deployed into the kube-system namespace. View the status of the deployment with kubectl get pods -n kube-system`

> A ServiceAccount is required to login. A ClusterRoleBinding is used to assign the new ServiceAccount
(_admin-user_) the role of _cluster-admin_ on the cluster.

```Shell
master $ cat <<EOF | kubectl create -f - 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system
EOF

```

This means they can control all aspects of Kubernetes. With ClusterRoleBinding and RBAC, different level of permissions
can be defined based on security requirements. More information on creating a user for the Dashboard can be found in
the [Dashboard documentation](https://github.com/kubernetes/dashboard/wiki/Creating-sample-user).

* Once the ServiceAccount has been created, the token to login can be found with:
```Shell
master $ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
```

> When the dashboard was deployed, it used externalIPs to bind the service to port 8443. This makes the dashboard
available to outside of the cluster and viewable at https://2886795372-8443-elsy04.environments.katacoda.com/

* Use the admin-user token to access the dashboard.

>For production, instead of externalIPs, it's recommended to use `kubectl proxy to access the dashboard. 
See more details at [https://github.com/kubernetes/dashboard](https://github.com/kubernetes/dashboard.).

### 6 - Creating a Deployment, Starting a Container

```
$ kubectl run http --image=katacoda/docker-http-server:latest --replicas=1
$ kubectl get deployments  # view status of deployments
$ kubectl describe deployment http  # view how many replicas are available,
## labels specified and the events associated with the deploymen
```

#### 6.1 - Exposing a Port

* Use the following command to expose the container port 80 on the host 8000 binding to the external-ip of the host.
```
# kubectl expose deployment http --external-ip="172.17.0.72" --port=8000 --target-port=80
```
> With kubectl run it's possible to create the deployment and expose it as a single command.

* Use the command below to create a second http service exposed on port 8001.
```
kubectl run httpexposed --image=katacoda/docker-http-server:latest --replicas=1 --port=80 --hostport=8001
```

>Under the covers, this exposes the Pod via Docker Port Mapping. As a result, you will not see the service listed using:
`kubectl get svc

> To find the details you can use `docker ps | grep httpexposed`

#### Pause Containers
Running the above command you'll notice the ports are exposed on the Pod, not the http container itself.
The Pause container is responsible for defining the network for the Pod. Other containers in the pod share the same
network namespace. This improves network performance and allow multiple containers to communicate over the same
network interface.

#### 6.2 - Scale Containers

The command kubectl scale allows us to adjust the number of Pods running for a particular deployment or replication
controller.
```
$ kubectl scale --replicas=3 deployment http
$ kubectl get pods
$ kubectl describe svc http  # view the endpoint and the associated Pods which are included.
```

### 7 - Deploy Containers Using YAML

#### 7.1 - Create Deployment

* Copy the following to a file named deployment.yaml to run _webapp1_ using the Docker Image katacoda/docker-http-server
that runs on Port 80.

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: webapp1
    spec:
      containers:
      - name: webapp1
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
```
This is deployed to the cluster with the command:

`$ kubectl create -f deployment.yaml`

As it's a Deployment object, a list of all the deployed objects can be obtained via:
 
`$ kubectl get deployment`

Details of individual deployments can be outputted with:

`kubectl describe deployment webapp1`

#### 7.2 - Create Service

The Service selects all applications with the label webapp1. As multiple replicas, or instances, are deployed,
they will be automatically load balanced based on this common label. The Service makes the application available
via a NodePort:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp1-svc
  labels:
    app: webapp1
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 30080
  selector:
    app: webapp1
```

Deploy the Service with `$ kubectl create -f service.yaml`


As before, details of all the Service objects deployed with `kubectl get svc`. 

By describing the object it's possible to discover more details about the configuration:
```
$ kubectl describe svc webapp1-svc
$ curl host01:30080
```

#### 7.3 - Scale Deployment

Update the deployment.yaml file to increase the number of instances running.
For example, the file should look like this:

> replicas: 4

* Updates to existing definitions are applied using _kubectl apply_. To scale the number of replicas, deploy
the updated YAML file using:
```
$ kubectl apply -f deployment.yaml
```

* Instantly, the desired state of our cluster has been updated, viewable with:
```
$ kubectl get deployment
```

Additional Pods will be scheduled to match the request:
```
$ kubectl get pods
```

> As all the Pods have the same label selector, they'll be load balanced behind the Service NodePort deployed.

* Issuing requests to the port will result in different containers processing the request `curl host01:30080`

### 8 - Networking

#### 8.1 - Cluster IP

> Cluster IP is the default approach when creating a Kubernetes Service. The service is allocated an internal IP that
other components can use to access the pods.

By having a single IP address it enables the service to be load balanced across multiple Pods.
```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp1-clusterip-svc
  labels:
    app: webapp1-clusterip
spec:
  ports:
  - port: 80
  selector:
    app: webapp1-clusterip
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1-clusterip-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: webapp1-clusterip
    spec:
      containers:
      - name: webapp1-clusterip-pod
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---
```
> This definition will deploy a web app with two replicas to showcase load balancing along with a service.
```
master $ kubectl get pods
NAME                                            READY   STATUS    RESTARTS   AGE
webapp1-clusterip-deployment-669c7c65c4-d9b9g   1/1     Running   0          2m18s
webapp1-clusterip-deployment-669c7c65c4-j6xwd   1/1     Running   0          2m18s

master $ kubectl get svc
NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
kubernetes              ClusterIP   10.96.0.1        <none>        443/TCP   2m17s
webapp1-clusterip-svc   ClusterIP   10.106.180.198   <none>        80/TCP    56s
```

* After deploying, the service can be accessed via the ClusterIP allocated:

```shell
export CLUSTER_IP=$(kubectl get services/webapp1-clusterip-svc -o go-template='{{(index .spec.clusterIP)}}')
echo CLUSTER_IP=$CLUSTER_IP
curl $CLUSTER_IP:80
```

#### 8.2 - Target Port

> Target ports allows us to separate the port the service is available on from the port the application is listening on.
TargetPort is the Port which the application is configured to listen on. Port is how the application will be accessed
from the outside.
```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp1-clusterip-targetport-svc
  labels:
    app: webapp1-clusterip-targetport
spec:
  ports:
  - port: 8080
    targetPort: 80
  selector:
    app: webapp1-clusterip-targetport
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1-clusterip-targetport-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: webapp1-clusterip-targetport
    spec:
      containers:
      - name: webapp1-clusterip-targetport-pod
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---
```

```
master $ kubectl get svc
NAME                               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
kubernetes                         ClusterIP   10.96.0.1        <none>        443/TCP    7m5s
webapp1-clusterip-targetport-svc   ClusterIP   10.102.82.108    <none>        8080/TCP   2s
```
* After the service and pods have deployed, it can be accessed via the cluster IP as before, but this time on the
defined port 8080.

#### 8.3 - NodePort

> While TargetPort and ClusterIP make it available to inside the cluster, the NodePort exposes the service on each
Nodeâ€™s IP via the defined static port. No matter which Node within the cluster is accessed, the service will be
reachable based on the port number defined.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp1-nodeport-svc
  labels:
    app: webapp1-nodeport
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 30080
  selector:
    app: webapp1-nodeport
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1-nodeport-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: webapp1-nodeport
    spec:
      containers:
      - name: webapp1-nodeport-pod
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---
```

```
master $ kubectl get svc
NAME                               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes                         ClusterIP   10.96.0.1        <none>        443/TCP        9m38s
webapp1-nodeport-svc               NodePort    10.98.13.126     <none>        80:30080/TCP   3s
```

The service can now be reached via the Node's IP address on the NodePort defined.

```shell
master $ curl 172.17.0.29:30080
<h1>This request was processed by host: webapp1-nodeport-deployment-677bd89b96-kst4b</h1>
master $ curl 172.17.0.29:30080
<h1>This request was processed by host: webapp1-nodeport-deployment-677bd89b96-srksr</h1>
```

#### 8.4 - External IPs
> Another approach to making a service available outside of the cluster is via External IP addresses.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp1-externalip-svc
  labels:
    app: webapp1-externalip
spec:
  ports:
  - port: 80
  externalIPs:
  - HOSTIP
  selector:
    app: webapp1-externalip
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1-externalip-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: webapp1-externalip
    spec:
      containers:
      - name: webapp1-externalip-pod
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---
```

* Update the definition to the current cluster's IP address with:

```shell
master $ sed -i 's/HOSTIP/172.17.0.29/g' externalip.yaml
master $ kubectl apply -f externalip.yaml
master $ kubectl get svc
NAME                               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes                         ClusterIP   10.96.0.1        <none>        443/TCP        13m
webapp1-externalip-svc             ClusterIP   10.99.87.109     172.17.0.29   80/TCP         2s
webapp1-nodeport-svc               NodePort    10.98.13.126     <none>        80:30080/TCP   4m7s
master $ curl 172.17.0.29
<h1>This request was processed by host: webapp1-externalip-deployment-6446b488f8-6lr2x</h1>
master $ curl 172.17.0.29
<h1>This request was processed by host: webapp1-externalip-deployment-6446b488f8-r6h9g</h1>
```

#### 8.5 - Load Balancer

> When running in the cloud, such as EC2 or Azure, it's possible to configure and assign a Public IP address issued
via the cloud provider. This will be issued via a Load Balancer such as ELB. This allows additional public IP addresses
to be allocated to a Kubernetes cluster without interacting directly with the cloud provider.

* As Katacoda is not a cloud provider, it's still possible to dynamically allocate IP addresses to LoadBalancer
type services. This is done by deploying the Cloud Provider using
`kubectl apply -f cloudprovider.yaml` (see the file as named as same in this dir)[./cloudprovider.yaml].
When running in a service provided by a Cloud Provider this is not required.

* When a service requests a Load Balancer, the provider will allocate one from the 10.10.0.0/26 range defined in the
configuration.

> see the pods named keepalived below, after applied cloudprovider.yaml

```
master $ kubectl get pods -n kube-system
NAME                                        READY   STATUS    RESTARTS   AGE
coredns-fb8b8dccf-4tcwg                     1/1     Running   0          22m
coredns-fb8b8dccf-tm9xp                     1/1     Running   0          22m
etcd-master                                 1/1     Running   0          21m
keepalived-cloud-provider-78fc4468b-9dq64   1/1     Running   0          4m49s
kube-apiserver-master                       1/1     Running   0          21m
kube-controller-manager-master              1/1     Running   0          21m
kube-keepalived-vip-dc45m                   1/1     Running   0          4m49s
kube-proxy-xpz5l                            1/1     Running   0          22m
kube-scheduler-master                       1/1     Running   0          21m
weave-net-cl6m4                             2/2     Running   0          22m

master $ kubectl apply -f loadbalancer.yaml
```
* _The service is configured via a Load Balancer as defined in:_

```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp1-loadbalancer-svc
  labels:
    app: webapp1-loadbalancer
spec:
  type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: webapp1-loadbalancer
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1-loadbalancer-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: webapp1-loadbalancer
    spec:
      containers:
      - name: webapp1-loadbalancer-pod
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---
```

```
master $ kubectl get svc
NAME                               TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes                         ClusterIP      10.96.0.1        <none>        443/TCP        26m
webapp1-externalip-svc             ClusterIP      10.99.87.109     172.17.0.29   80/TCP         12m
webapp1-loadbalancer-svc           LoadBalancer   10.98.66.206     10.10.0.1     80:30964/TCP   101s
webapp1-nodeport-svc               NodePort       10.98.13.126     <none>        80:30080/TCP   16m
```
* The service can now be accessed via the IP address assigned, in this case from the 10.10.0.0/26 range.

```shell
master $ export LoadBalancerIP=$(kubectl get services/webapp1-loadbalancer-svc -o go-template='{{(index .status.loadBalancer.ingress 0).ip}}')
master $ echo LoadBalancerIP=$LoadBalancerIP
LoadBalancerIP=10.10.0.1
master $ curl $LoadBalancerIP
<h1>This request was processed by host: webapp1-loadbalancer-deployment-f45b8d9cd-p2k6d</h1>
master $ curl $LoadBalancerIP
<h1>This request was processed by host: webapp1-loadbalancer-deployment-f45b8d9cd-c5pqw</h1>
```

